import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import joblib
import os

def step3_representation_and_split():
    """Ã‰TAPE 3: ReprÃ©sentation vectorielle (TF-IDF) et division des donnÃ©es"""
    
    print("ðŸ”¢ Ã‰TAPE 3: REPRÃ‰SENTATION VECTORIELLE (TF-IDF)")
    print("="*60)
    
    # 1. CHARGEMENT DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES
    base_dir = os.path.dirname(os.path.dirname(__file__))
    data_path = os.path.join(base_dir, 'data', 'processed', 'tiktok_preprocessed.csv')
    
    df = pd.read_csv(data_path)
    
    # Supprimer les lignes avec du texte vide (juste au cas oÃ¹)
    df.dropna(subset=['text_processed'], inplace=True)
    
    print(f"ðŸ“Š Dataset chargÃ©: {len(df):,} avis")
    
    # 2. PRÃ‰PARATION DES DONNÃ‰ES (X et y)
    print("\nðŸ”„ PrÃ©paration des donnÃ©es pour la modÃ©lisation...")
    
    # X: Le texte traitÃ© (nos features)
    X = df['text_processed']
    
    # y: Les sentiments (nos cibles)
    y = df['sentiment']
    
    print(f"   â€¢ X (textes): {X.shape[0]} exemples")
    print(f"   â€¢ y (sentiments): {y.shape[0]} labels")
    
    # 3. CRÃ‰ATION DE LA MATRICE TF-IDF
    print("\nðŸ”  CrÃ©ation des features TF-IDF...")
    
    # Initialiser le vectoriseur TF-IDF
    vectorizer = TfidfVectorizer(
        max_features=5000,      # Garder les 5000 mots les plus importants
        ngram_range=(1, 2),     # ConsidÃ©rer les mots seuls (unigrammes) et les paires de mots (bigrammes)
        min_df=5                # Ignorer les mots qui apparaissent dans moins de 5 avis
    )
    
    # Appliquer le vectoriseur sur nos textes
    X_tfidf = vectorizer.fit_transform(X)
    
    print(f"âœ… Matrice TF-IDF crÃ©Ã©e avec succÃ¨s!")
    print(f"   â€¢ Forme de la matrice: {X_tfidf.shape[0]} lignes Ã— {X_tfidf.shape[1]} features")
    print(f"   â€¢ Nombre de mots dans le vocabulaire: {len(vectorizer.get_feature_names_out())}")
    
    # Afficher quelques features
    print(f"   â€¢ Exemples de features: {list(vectorizer.get_feature_names_out())[2000:2010]}")
    
    # 4. DIVISION DES DONNÃ‰ES (Train / Test)
    print("\nðŸ”ª Division des donnÃ©es en ensembles d'entraÃ®nement et de test...")
    
    # 80% pour l'entraÃ®nement, 20% pour le test
    X_train, X_test, y_train, y_test = train_test_split(
        X_tfidf, 
        y, 
        test_size=0.2, 
        random_state=42,
        stratify=y  # Important pour garder la mÃªme proportion de sentiments dans train et test
    )
    
    print(f"âœ… DonnÃ©es divisÃ©es:")
    print(f"   â€¢ EntraÃ®nement: {X_train.shape[0]:,} exemples")
    print(f"   â€¢ Test: {X_test.shape[0]:,} exemples")
    
    # 5. SAUVEGARDER LES DONNÃ‰ES ET LE VECTORIZER
    print("\nðŸ’¾ Sauvegarde des donnÃ©es et du vectorizer pour l'Ã©tape 4...")
    
    output_dir = os.path.join(base_dir, 'data', 'processed')
    
    # Sauvegarder les ensembles de donnÃ©es
    joblib.dump(X_train, os.path.join(output_dir, 'X_train.pkl'))
    joblib.dump(X_test, os.path.join(output_dir, 'X_test.pkl'))
    joblib.dump(y_train, os.path.join(output_dir, 'y_train.pkl'))
    joblib.dump(y_test, os.path.join(output_dir, 'y_test.pkl'))
    
    # Sauvegarder le vectorizer (trÃ¨s important pour l'API)
    models_dir = os.path.join(base_dir, 'data', 'models')
    os.makedirs(models_dir, exist_ok=True)
    joblib.dump(vectorizer, os.path.join(models_dir, 'tfidf_vectorizer.pkl'))
    
    print(f"   â€¢ Fichiers de donnÃ©es sauvegardÃ©s dans: {output_dir}")
    print(f"   â€¢ Vectorizer sauvegardÃ© dans: {models_dir}")
    
    print(f"\nðŸŽ¯ PrÃªt pour l'Ã‰TAPE 4: CLASSIFICATION")
    
    return True

if __name__ == "__main__":
    step3_representation_and_split()