import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import emoji
import os
from collections import Counter
import matplotlib.pyplot as plt

# TÃ©lÃ©charger les ressources NLTK nÃ©cessaires
def setup_nltk():
    """TÃ©lÃ©charge les ressources NLTK nÃ©cessaires"""
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('corpora/stopwords')
        nltk.data.find('corpora/wordnet')
    except LookupError:
        print(" TÃ©lÃ©chargement des ressources NLTK...")
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
        nltk.download('omw-1.4', quiet=True)

class TextPreprocessor:
    
    def __init__(self):
        setup_nltk()
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        self.tiktok_words = {'tiktok', 'app', 'video', 'content', 'creator', 'fyp', 'viral'}
        self.stop_words = self.stop_words - self.tiktok_words
        
    def clean_text(self, text):
        """Nettoie le texte des avis"""
        if pd.isna(text):
            return ""
        
        text = str(text)
        
        # 1. Convertir les emojis en texte
        text = emoji.demojize(text, delimiters=(" ", " "))
        
        # 2. Convertir en minuscules
        text = text.lower()
        
        # 3. Supprimer les URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\$$\$$,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # 4. Supprimer les mentions (@username)
        text = re.sub(r'@\w+', '', text)
        
        # 5. Supprimer les hashtags mais garder le texte
        text = re.sub(r'#(\w+)', r'\1', text)
        
        # 6. Supprimer les caractÃ¨res spÃ©ciaux mais garder la ponctuation importante
        text = re.sub(r'[^\w\s!?.]', ' ', text)
        
        # 7. Supprimer les chiffres isolÃ©s
        text = re.sub(r'\b\d+\b', '', text)
        
        # 8. Supprimer les espaces multiples
        text = ' '.join(text.split())
        
        return text.strip()
    
    def tokenize_and_lemmatize(self, text):
        """Tokenise et lemmatise le texte"""
        if not text:
            return []
        
        # Tokenisation
        tokens = word_tokenize(text)
        
        # Supprimer la ponctuation et les mots courts
        tokens = [token for token in tokens if token not in string.punctuation and len(token) > 2]
        
        # Supprimer les stop words
        tokens = [token for token in tokens if token not in self.stop_words]
        
        # Lemmatisation
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
        
        return tokens
    
    def preprocess_text(self, text):
        """Pipeline complet de prÃ©traitement"""
        # Nettoyage
        cleaned = self.clean_text(text)
        
        # Tokenisation et lemmatisation
        tokens = self.tokenize_and_lemmatize(cleaned)
        
        # Rejoindre les tokens
        processed = ' '.join(tokens)
        
        return processed

def step2_preprocessing():
    """Ã‰TAPE 2: PrÃ©traitement des textes d'avis TikTok"""
    
    print("ğŸ§¹ Ã‰TAPE 2: PRÃ‰TRAITEMENT DES TEXTES")
    print("="*60)
    base_dir = os.path.dirname(os.path.dirname(__file__))
    data_path = os.path.join(base_dir, 'data', 'processed', 'tiktok_sentiment_data.csv')
    
    df = pd.read_csv(data_path)
    print(f"ğŸ“Š Dataset chargÃ©: {len(df):,} avis")
    sample_size = min(50000, len(df))  # Limiter Ã  50k pour les tests
    df_sample = df.sample(n=sample_size, random_state=42)
    print(f"ğŸ“ Ã‰chantillon de travail: {len(df_sample):,} avis")
    
    preprocessor = TextPreprocessor()
    print(f"\n EXEMPLES DE PRÃ‰TRAITEMENT:")
    
    sample_texts = [
        "I LOVE THIS APP!!! ğŸ˜ğŸ˜ğŸ˜ Best app ever!!!",
        "This app is terrible... so many bugs ğŸ˜¡",
        "TikTok is okay, not bad but could be better ğŸ¤·â€â™€ï¸",
        "Can't even sign in!!! Fix your app @tiktok #frustrated",
        "Great content and videos! Very addictive app ğŸ”¥"
    ]
    
    for i, text in enumerate(sample_texts, 1):
        cleaned = preprocessor.clean_text(text)
        processed = preprocessor.preprocess_text(text)
        
        print(f"\n   {i}. Original: {text}")
        print(f"      NettoyÃ©:  {cleaned}")
        print(f"      TraitÃ©:   {processed}")
    
    # 2. TRAITEMENT DE TOUT LE DATASET
    print(f"\nğŸ”„ Traitement de l'Ã©chantillon...")
    
    # Ajouter les versions nettoyÃ©es et traitÃ©es
    df_sample['text_cleaned'] = df_sample['text'].apply(preprocessor.clean_text)
    df_sample['text_processed'] = df_sample['text'].apply(preprocessor.preprocess_text)
    
    # Supprimer les textes vides aprÃ¨s traitement
    df_clean = df_sample[df_sample['text_processed'].str.len() > 0].copy()
    print(f"âœ… AprÃ¨s nettoyage: {len(df_clean):,} avis utilisables")
    
    # 3. STATISTIQUES POST-TRAITEMENT
    print(f"\nğŸ“Š STATISTIQUES POST-TRAITEMENT:")
    
    # Longueur des textes
    original_lengths = df_clean['text'].str.len()
    processed_lengths = df_clean['text_processed'].str.len()
    
    print(f"   ğŸ“ Longueur moyenne:")
    print(f"      â€¢ Originale: {original_lengths.mean():.1f} caractÃ¨res")
    print(f"      â€¢ TraitÃ©e:   {processed_lengths.mean():.1f} caractÃ¨res")
    
    # Distribution par sentiment
    print(f"\n   ğŸ˜Š Distribution finale:")
    sentiment_dist = df_clean['sentiment'].value_counts()
    for sentiment, count in sentiment_dist.items():
        percentage = (count / len(df_clean)) * 100
        print(f"      â€¢ {sentiment}: {count:,} ({percentage:.1f}%)")
    
    # 4. ANALYSE DES MOTS LES PLUS FRÃ‰QUENTS
    print(f"\nğŸ”¤ MOTS LES PLUS FRÃ‰QUENTS PAR SENTIMENT:")
    
    for sentiment in ['positive', 'negative', 'neutral']:
        texts = df_clean[df_clean['sentiment'] == sentiment]['text_processed']
        all_words = ' '.join(texts).split()
        
        if all_words:
            word_freq = Counter(all_words)
            top_words = word_freq.most_common(10)
            
            print(f"\n   ğŸ“± {sentiment.upper()}:")
            for word, freq in top_words:
                print(f"      â€¢ {word}: {freq:,}")
    
    # 5. SAUVEGARDER LES DONNÃ‰ES PRÃ‰TRAITÃ‰ES
    output_path = os.path.join(base_dir, 'data', 'processed', 'tiktok_preprocessed.csv')
    
    # Garder les colonnes importantes
    df_final = df_clean[['text', 'text_cleaned', 'text_processed', 'sentiment', 'score']].copy()
    df_final.to_csv(output_path, index=False, encoding='utf-8')
    
    print(f"\n DONNÃ‰ES PRÃ‰TRAITÃ‰ES SAUVEGARDÃ‰ES:")
    print(f"    Fichier: {output_path}")
    print(f"    Dataset final: {len(df_final):,} avis")
    print(f"    PrÃªt pour l'Ã‰TAPE 3: REPRÃ‰SENTATION")
    
    return df_final

if __name__ == "__main__":
    df = step2_preprocessing()
